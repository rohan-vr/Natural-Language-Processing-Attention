{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## The goal of this notebook is to introduce sequence to sequence language translation (seq2seq) and Attention mechanism.\nThe notebook deals with a sequence to sequence model for English to Hindi translation. After training the model one will be able to input a English sentence and get back its Hindi translation.\n\n>RNNs are also capable of doing natural language translation, aka. machine translation. It involves two RNNs, one for the source language and one for the target language. One of them is called an encoder, and the other one decoder. The reason is that, the first one encodes the sentence into a vector and the second one converts the encoded vector into a sentence in target language. The decoder is a separete RNN. Given the encoded sentence, it produces the translated sentence in target language. Attention lets the decoder to focus on specific parts of the input sentence for each output word. This helps the input and output sentences to align with one another.\n\nWe obtained the dataset used from Kaggle: https://www.kaggle.com/aiswaryaramachandran/hindienglish-corpora\n\n<h2> References: </h2>\n<li></a> Sequence to Sequence Learning with Neural Networks (Research Publication)</li>\n<li></a> https://www.tensorflow.org/tutorials/text/nmt_with_attention </li>\n<li></a> Using stochastic computation graphs formalism for optimization of sequence-to-sequence model (Research Publication) </li>\n</ul>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport string\nimport numpy as np\nimport pandas as pd\nfrom string import digits\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nimport logging\nimport tensorflow as tf\ntf.enable_eager_execution()\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nlogging.getLogger('tensorflow').setLevel(logging.FATAL)\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\nimport unicodedata\nimport io\nimport time\nimport warnings\nimport sys\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nPATH = \"../input/combination/mixsen_csv.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess English and Hindi sentences","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn')\n\ndef preprocess_sentence(w):\n    w = unicode_to_ascii(w.lower().strip())\n    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n    w = w.rstrip().strip()\n    return w\n\ndef hindi_preprocess_sentence(w):\n    w = w.rstrip().strip()\n    return w","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataset(path=PATH):\n    lines=pd.read_csv(path,encoding='utf-8')\n    en = []\n    hd = []\n    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n        print(i)\n        print(j)\n        if len(i.split(' '))<=20 and len(j.split(' '))<=20:\n            en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n            en_1.append('<end>')\n            en_1.insert(0, '<start>')\n            hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n            hd_1.append('<end>')\n            hd_1.insert(0, '<start>')\n            en.append(en_1)\n            hd.append(hd_1)\n    return hd, en","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def max_length(tensor):\n    return max(len(t) for t in tensor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenization of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(lang):\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n  lang_tokenizer.fit_on_texts(lang)\n  tensor = lang_tokenizer.texts_to_sequences(lang)\n  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n  return tensor, lang_tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset(path=PATH):\n    targ_lang, inp_lang = create_dataset(path)\n    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\nmax_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Train and Test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\nprint(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert(lang, tensor):\n  for t in tensor:\n    if t!=0:\n      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n    \nprint (\"Input Language; index to word mapping\")\nconvert(inp_lang, input_tensor_train[0])\nprint ()\nprint (\"Target Language; index to word mapping\")\nconvert(targ_lang, target_tensor_train[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Dataset\n> We are using minimal configuration as the notebbok is not focussed on metrics performance but rather the implementation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"BUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 32\nsteps_per_epoch = len(input_tensor_train)//BATCH_SIZE\nembedding_dim = 512\nunits = 512\nvocab_inp_size = len(inp_lang.word_index)+1\nvocab_tar_size = len(targ_lang.word_index)+1\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoder Decoder with Attention Model\n\n> Encoder Decoder with Attention model is a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. It uses a multilayered Gated Recurrent Unit (GRU) to map the input sequence to a vector of a fixed dimensionality, and then another deep GRU to decode the target sequence from the vector.\n<img src=\"https://www.researchgate.net/profile/Vlad_Zhukov2/publication/321210603/figure/fig1/AS:642862530191361@1530281779831/An-example-of-sequence-to-sequence-model-with-attention-Calculation-of-cross-entropy.png\" width=\"800\" alt=\"attention mechanism\">\n\n> A sequence to sequence model has two parts – an encoder and a decoder. Both the parts are practically two different neural network models combined into one giant network. the task of an encoder network is to understand the input sequence, and create a smaller dimensional representation of it. This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output. The input is put through an encoder model which gives us the encoder output. Here, each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. We use Bahdanau attention for the encoder.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Encoder","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n    super(Encoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n  def call(self, x, hidden):\n    x = self.embedding(x)\n    output, state = self.gru(x, initial_state = hidden)\n    return output, state\n\n  def initialize_hidden_state(self):\n    return tf.zeros((self.batch_sz, self.enc_units))\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Attention Mechanism","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class BahdanauAttention(tf.keras.layers.Layer):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, query, values):\n    hidden_with_time_axis = tf.expand_dims(query, 1)\n    score = self.V(tf.nn.tanh(\n        self.W1(values) + self.W2(hidden_with_time_axis)))\n    attention_weights = tf.nn.softmax(score, axis=1)\n    context_vector = attention_weights * values\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n    return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decoder","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n    super(Decoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.dec_units = dec_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc = tf.keras.layers.Dense(vocab_size)\n    self.attention = BahdanauAttention(self.dec_units)\n\n  def call(self, x, hidden, enc_output):\n    context_vector, attention_weights = self.attention(hidden, enc_output)\n    x = self.embedding(x)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n    output, state = self.gru(x)\n    output = tf.reshape(output, (-1, output.shape[2]))\n    x = self.fc(output)\n    return x, state, attention_weights\n\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n  mask = tf.cast(mask, dtype=loss_.dtype)\n#   print(type(mask))\n  loss_ *= mask\n  return tf.reduce_mean(loss_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\n\n>1. Pass *input* through *encoder* to get *encoder output*..\n>2. Then encoder output, encoder hidden state and the decoder input is passed to decoder.\n>3. Decoder returns *predictions* and *decoder hidden state*.\n>4. Decoder hidden state is then passed back to model.\n>5. Predictions are used to calculate loss.\n>6. Use *teacher forcing* (technique where the target word is passed as the next input to the decoder) for the next input to the decoder.\n>7. Calculate gradients and apply it to *optimizer* for backpropogation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n  loss = 0\n  with tf.GradientTape() as tape:\n    enc_output, enc_hidden = encoder(inp, enc_hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n    # Teacher forcing\n    for t in range(1, targ.shape[1]):\n      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n      loss += loss_function(targ[:, t], predictions)\n      dec_input = tf.expand_dims(targ[:, t], 1)\n\n  batch_loss = (loss / int(targ.shape[1]))\n  variables = encoder.trainable_variables + decoder.trainable_variables\n  gradients = tape.gradient(loss, variables)\n  optimizer.apply_gradients(zip(gradients, variables))      \n  return batch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 30\n\nplot_loss=[]\n\nfor epoch in range(EPOCHS):\n  start = time.time()\n  enc_hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n    batch_loss = train_step(inp, targ, enc_hidden)\n    total_loss += batch_loss\n    if batch % 100 == 0:\n        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                     batch,\n                                                     batch_loss.numpy()))\n  if (epoch + 1) % 2 == 0:\n    checkpoint.save(file_prefix = checkpoint_prefix)\n\n  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss / steps_per_epoch))\n  plot_loss.append(total_loss/steps_per_epoch)\n  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n\nimport matplotlib.pyplot as plt\nplt.plot(plot_loss)\nplt.legend(['validation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(sentence):\n    attention_plot = np.zeros((max_length_targ, max_length_inp))\n    sentence = preprocess_sentence(sentence)\n    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                           maxlen=max_length_inp,\n                                                           padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    result = ''\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                             dec_hidden,\n                                                             enc_out)\n        predicted_id = tf.argmax(predictions[0]).numpy()\n        result += targ_lang.index_word[predicted_id] + ' '\n        if targ_lang.index_word[predicted_id] == '<end>':\n            return result, sentence\n        dec_input = tf.expand_dims([predicted_id], 0)\n    return result, sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\n# restoring the latest checkpoint in checkpoint_dir\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n\nsentences={'Phone':\n['Block calls from Jacob Andy right away',\n'Call 917 886-5091 on speaker',\n 'Edit the time of the USSR time zone reminder to go off every year in December',\n 'Connect me to the work number of Michael Smith using speakerphone'\n],\n\n'Reminder':\n['Create a reminder for every Wednesday at 1:00pm for waking up',\n\"Go to reminders and find Monica's birthday reminder\",\n 'Are there any reminders for this Thursday containing the word Manchester united game'\n],\n\n'Weather':\n[\"Show this week's weather forecast for Toledo Ohio on the master bedroom TV\",\n'Tell us the low temp in vegas next friday',\n'Show the apparent temperature this afternoon in Libertyville'],\n\n'Launcher':\n['Quit all background applications',\n'Bixby who called me today']}\n\ntranslated=[]\nactual=['ब्लॉक याकूब एंडी से तुरंत कॉल करें',\n        'स्पीकर पर 917 886-5091 पर कॉल करें',\n        'यूएसएसआर समय क्षेत्र अनुस्मारक के समय को हर साल दिसंबर में बंद करने के लिए संपादित करें',\n        'स्पीकरफ़ोन का उपयोग करके मुझे माइकल स्मिथ के कार्य संख्या से कनेक्ट करें',\n        'जागने के लिए हर बुधवार दोपहर 1:00 बजे के लिए एक अनुस्मारक बनाएँ',\n        'रिमाइंडर पर जाएं और मोनिका के जन्मदिन की याद दिलाएं',\n        'क्या इस गुरुवार के लिए कोई अनुस्मारक है जिसमें मैनचेस्टर एकजुट खेल शब्द है',\n        'मास्टर बेडरूम टीवी पर टोलेडो ओहियो के लिए इस सप्ताह के मौसम का पूर्वानुमान दिखाएं',\n        'हमें अगले शुक्रवार को वेजेस में कम अस्थायी बताएं',\n        'लिबर्टीविले में आज दोपहर को स्पष्ट तापमान दिखाएं',\n        'सभी पृष्ठभूमि अनुप्रयोगों से बाहर निकलें',\n        'बिक्सबी जिसने मुझे आज बुलाया है'\n       ]\n\ndef translate(sentence):\n    result, sentence = evaluate(sentence)\n    print('Input: %s' % (sentence))\n    print('Predicted translation: {}'.format(result))\n    translated.append(result)\n\n\nfor domians in sentences.keys():\n    text=sentences[domians]\n    for lines in text:\n        translate(lines)\n\n\nprint(corpus_bleu(actual, translated, weights=(1.0, 0, 0, 0)))\nprint(corpus_bleu(actual, translated, weights=(0.5, 0.5, 0, 0)))\nprint(corpus_bleu(actual, translated, weights=(0.3, 0.3, 0.3, 0)))\nprint(corpus_bleu(actual, translated, weights=(0.25, 0.25, 0.25, 0.25)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"translate(u'')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}